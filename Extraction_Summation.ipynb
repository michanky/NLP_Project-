{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Written here is the code for extraction summarization for articles. In extraction-based summarization, a subset of words that represent the most important points is pulled from a piece of text and combined to make a summary. In ML, extractive summarization involves weighing the essential sections of sentences and using the results to generate summaries. We will use frequency of important words as weights.If the sentence contains the most number of important words, we will assume that this is the best sentence for summary. Adapted from <a href=https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/>Floydhub blog</a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are five steps to do for an extraction summarization.\n",
    "<ol>\n",
    "    <li>Prepare Data</li>\n",
    "    <li>Process text</li>\n",
    "    <li>Tokenize text</li>\n",
    "    <li>Evaluate the weighted occurence frequency of the words</li>\n",
    "    <li>Substitute the words with the weighted frequency</li>\n",
    "</ol>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import bs4 as BeautifulSoup\r\n",
    "import urllib.request  \r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\r\n",
    "import nltk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Prepare Data</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Fetching the content from the URL\r\n",
    "fetched_data = urllib.request.urlopen('https://www.moh.gov.sg/news-highlights/details/preparing-for-our-transition-towards-covid-resilience')\r\n",
    "\r\n",
    "article_read = fetched_data.read()\r\n",
    "\r\n",
    "# Parsing the URL content and storing in a variable\r\n",
    "article_parsed = BeautifulSoup.BeautifulSoup(article_read,'html.parser')\r\n",
    "\r\n",
    "\r\n",
    "# Returning <p> tags\r\n",
    "paragraphs = article_parsed.find_all('p')\r\n",
    "\r\n",
    "article_content = ''\r\n",
    "\r\n",
    "# Looping through the paragraphs and adding them to the variable\r\n",
    "for p in paragraphs:  \r\n",
    "    article_content += p.text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What is done here is fetching and reading the whole webpage, afterward we parse the whole webpage and extract all paragraphs. We will then append all paragraphs into a var called article_content."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Processing Data</h1>"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def create_dictionary_table(text) -> dict:\r\n",
    "    stop_words = set(stopwords.words('english'))\r\n",
    "    \r\n",
    "    #tokenize the words\r\n",
    "    words = word_tokenize(text)\r\n",
    "    \r\n",
    "    #reducing words to root form\r\n",
    "    stem = PorterStemmer()\r\n",
    "    \r\n",
    "    #Create dictionary for the word frequency table\r\n",
    "    frequency_table = {}\r\n",
    "    \r\n",
    "    for wd in words:\r\n",
    "        wd = stem.stem(wd)\r\n",
    "        if wd in stop_words:\r\n",
    "            continue\r\n",
    "        if wd in frequency_table:\r\n",
    "            frequency_table[wd] += 1\r\n",
    "        else:\r\n",
    "            frequency_table[wd] = 1\r\n",
    "    return frequency_table   "
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Tokenize the text </h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#sent_tokenize()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Evaluate the weighted occurence frequency of the words</h1>"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def calculate_sentence_scores(sentences,frequency_table) -> dict:\r\n",
    "    sentence_weight = dict()\r\n",
    "    \r\n",
    "    for sentence in sentences:\r\n",
    "        sentence_wordcount = len(word_tokenize(sentence))\r\n",
    "        sentence_wordcount_without_stop_words = 0 \r\n",
    "        for word_weight in frequency_table:\r\n",
    "            if word_weight in sentence.lower():\r\n",
    "                sentence_wordcount_without_stop_words += 1\r\n",
    "                if sentence[:7] in sentence_weight:\r\n",
    "                    sentence_weight[sentence[:7]] += frequency_table[word_weight]\r\n",
    "                else:\r\n",
    "                    sentence_weight[sentence[:7]] = frequency_table[word_weight]\r\n",
    "                    \r\n",
    "        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] / sentence_wordcount_without_stop_words\r\n",
    "    return sentence_weight"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Calculate the threshold of each sentence </h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def calculate_average_score(sentence_weight):\r\n",
    "    sum_values = 0 \r\n",
    "    for entry in sentence_weight:\r\n",
    "        sum_values += sentence_weight[entry]\r\n",
    "    \r\n",
    "    avg_score = (sum_values/len(sentence_weight))\r\n",
    "    return avg_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def get_article_summary(sentences, sentence_weight, threshold):\r\n",
    "    sentence_counter = 0\r\n",
    "    article_summary = ''\r\n",
    "\r\n",
    "    for sentence in sentences:\r\n",
    "        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\r\n",
    "            article_summary += \" \" + sentence\r\n",
    "            sentence_counter += 1\r\n",
    "\r\n",
    "    return article_summary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1> Wrapping everything up </h1>"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "frequency_table = create_dictionary_table(article_content)\r\n",
    "sentences = sent_tokenize(article_content)\r\n",
    "sentence_weight = calculate_sentence_scores(sentences,frequency_table)\r\n",
    "avg_score = calculate_average_score(sentence_weight)\r\n",
    "\r\n",
    "summary = get_article_summary(sentences, sentence_weight, 1.5*avg_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "summary"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' Click here for E-Consultation. Hence, we have ramped up efforts to encourage more seniors to be vaccinated. Many of our seniors have responded to these initiatives. But there are still about 80,000 in this group that have yet to be vaccinated. Hence, we have ramped up efforts to encourage more seniors to be vaccinated. Many of our seniors have responded to these initiatives. But there are still about 80,000 in this group that have yet to be vaccinated. solemnizations, congregational and other worship services. solemnizations, congregational and other worship services.11. These events may take place with up to 1,000 attendees if all are fully vaccinated. These events may take place with up to 1,000 attendees if all are fully vaccinated. Work-from-home. Work-from-home. Hence we will require vaccination, or regular testing in lieu, for selected sectors of the workforce. Hence we will require vaccination, or regular testing in lieu, for selected sectors of the workforce.'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('personal': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "7a42ef9efc5686d968eda5ac85e4cfe02490e28b871f845a3d535a30d69d90fb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}